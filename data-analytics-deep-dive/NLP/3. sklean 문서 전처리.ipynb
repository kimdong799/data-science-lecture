{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confirmed-alias",
   "metadata": {},
   "source": [
    "# sklearn 문서 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-draft",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-blowing",
   "metadata": {},
   "source": [
    "BOW(Bag of Words) 인코딩 방법 : 문서를 숫자 벡터로 변환하는 가장 기본적인 방법, BOW 인코딩 방법은 전체 문서 $[d_1, d_2, ..., d_n]$을 구성하는 고정된 단어장(vocabulary) $[t_1, t_2, ..., t_n]$을 만들고 해당 단어들이 포함되어 있는지를 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-snake",
   "metadata": {},
   "source": [
    "표현 방법 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-solomon",
   "metadata": {},
   "source": [
    "$x_{i,j} = 문서 d_i 내의 단어 t_i의 출현 빈도$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-millennium",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:01:29.214422Z",
     "start_time": "2021-08-30T03:01:29.194493Z"
    }
   },
   "source": [
    "표현 방법 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-reunion",
   "metadata": {},
   "source": [
    "$\\begin{split} x_{i,j} = \n",
    "\\begin{cases}\n",
    "0, & \\text{만약 단어 $t_j$가 문서 $d_i$ 안에 없으면} \\\\\n",
    "1. & \\text{만약 단어 $t_j$가 문서 $d_i$ 안에 있으면}\n",
    "\\end{cases}\n",
    "\\end{split}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-press",
   "metadata": {},
   "source": [
    "# 문서 전처리용 클래스의 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-administrator",
   "metadata": {},
   "source": [
    "**DictVectorizer**:\n",
    "\n",
    "각 단어의 수를 세어놓은 사전에서 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "**CountVectorizer**:\n",
    "\n",
    "문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "**TfidfVectorizer**:\n",
    "\n",
    "CountVectorizer와 비슷하지만 TF-IDF 방식으로 단어의 가중치를 조정한 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "**HashingVectorizer**:\n",
    "\n",
    "해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 BOW 인코딩 벡터를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-digest",
   "metadata": {},
   "source": [
    "## DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-daisy",
   "metadata": {},
   "source": [
    "DictVectorizer는 문서에서 단어의 사용 빈도를 나타내는 딕셔너리 정보를 입력받아 BOW를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-pixel",
   "metadata": {},
   "source": [
    "**DictVectorizer를 사용하기 위해선 단어의 출현빈도 정보를 가지고 있는 딕셔너리가 준비되어야한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "considerable-litigation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:06:42.702317Z",
     "start_time": "2021-08-30T03:06:42.685350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0.],\n",
       "       [0., 3., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "    # 1번 문서\n",
    "D = [{'Apple':1, 'Banana':2}, \n",
    "    # 2번 문서\n",
    "     {'Banana':3, 'Melon':1}]\n",
    "\n",
    "X = v.fit_transform(D)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-diesel",
   "metadata": {},
   "source": [
    "최종적으로 X행렬은 각 단어 Apple, Banana, Melon의 출현 빈도를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-switch",
   "metadata": {},
   "source": [
    "DictVectorizer 객체는 feature_names_ 속성에 fit된 단어들에 대한 정보를 담고있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proprietary-works",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:07:52.234763Z",
     "start_time": "2021-08-30T03:07:52.227783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', 'Banana', 'Melon']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_names_ 속성\n",
    "\n",
    "v.feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-candy",
   "metadata": {},
   "source": [
    "추가적으로 fit 에 사용하지 않은 단어가 추가적으로 입력되면 아예 없는 것으로 처리되기 때문에 되도록이면 vectorizer 객체에 단어들을 fit할 때 가능한 모든 단어를 사용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "middle-colon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:10:09.480255Z",
     "start_time": "2021-08-30T03:10:09.462266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform({'Melon':4, 'Orange':10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-portland",
   "metadata": {},
   "source": [
    "실제로 transform 메소드로 Melon과 Orange 정보를 넘기면 Orange에 대한 벡터화는 이루어지지 않는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-danish",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-burlington",
   "metadata": {},
   "source": [
    "CountVectorizer는 DictVectorizer에 fit하기 위한 단어의 출현빈도 정보를 가진 딕셔너리를 생성하는 기능을 제공한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-expense",
   "metadata": {},
   "source": [
    "1. 문서를 토큰 리스트로 변환\n",
    "2. 각 문서의 토큰 출현빈도 카운팅\n",
    "3. 각 문서를 BOW 인코딩 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "official-mason",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:13:22.962583Z",
     "start_time": "2021-08-30T03:13:22.943639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'is': 3,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'third': 8,\n",
       " 'one': 5,\n",
       " 'last': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',\n",
    "]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "\n",
    "# 단어와 index 번호 확인\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "official-timing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:14:29.117991Z",
     "start_time": "2021-08-30T03:14:29.107053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is the second document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "monetary-citizen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:14:38.060851Z",
     "start_time": "2021-08-30T03:14:38.054866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "endless-fancy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:14:49.917051Z",
     "start_time": "2021-08-30T03:14:49.905084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-navigator",
   "metadata": {},
   "source": [
    "**CountVectorizer에서 사용 가능한 파라미터 정보**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-armstrong",
   "metadata": {},
   "source": [
    "stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "\n",
    "stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "\n",
    "analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "\n",
    "단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "\n",
    "token_pattern : string\n",
    "\n",
    "토큰 정의용 정규 표현식\n",
    "\n",
    "tokenizer : 함수 또는 None (디폴트)\n",
    "\n",
    "토큰 생성 함수 .\n",
    "\n",
    "ngram_range : (min_n, max_n) 튜플\n",
    "\n",
    "n-그램 범위\n",
    "\n",
    "max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "\n",
    "단어장에 포함되기 위한 최대 빈도\n",
    "\n",
    "min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "\n",
    "단어장에 포함되기 위한 최소 빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-collapse",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-proxy",
   "metadata": {},
   "source": [
    "Stop Words는 문서에서 단어장을 생성할 때 무시할 단어이다. (불용어)\n",
    "\n",
    "일반적으로 영어의 관사, 접속사, 한국어의 조사 등이 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ordered-vegetarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:19:24.120473Z",
     "start_time": "2021-08-30T03:19:24.100522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=['and', 'is', 'the', 'this']).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "graphic-wales",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:19:35.594906Z",
     "start_time": "2021-08-30T03:19:35.580944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-entrance",
   "metadata": {},
   "source": [
    "# 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-terror",
   "metadata": {},
   "source": [
    "analyzer, tokenizer, token_pattern 파라미터로 사용할 토큰 생성기를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "medical-spencer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:20:43.060690Z",
     "start_time": "2021-08-30T03:20:43.050717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자로 토큰화\n",
    "\n",
    "vect = CountVectorizer(analyzer='char').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "architectural-teaching",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:20:53.700551Z",
     "start_time": "2021-08-30T03:20:53.685591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규표현식 이용\n",
    "# t로 시작하고 알파벳인 단어만 토큰으로 사용\n",
    "\n",
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cellular-steam",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:23:26.726300Z",
     "start_time": "2021-08-30T03:23:26.706321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 5,\n",
       " 'the': 9,\n",
       " 'first': 4,\n",
       " 'document': 3,\n",
       " '.': 0,\n",
       " 'second': 8,\n",
       " 'and': 2,\n",
       " 'third': 10,\n",
       " 'one': 7,\n",
       " '?': 1,\n",
       " 'last': 6}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 외부의 nltk, konlpy등의 좋은 토크나이저를 사용하고 싶은 경우 \n",
    "# 함수 자체를 인수로 넣어도 무방하다.\n",
    "\n",
    "import nltk\n",
    "\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-glass",
   "metadata": {},
   "source": [
    "# N그램"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-morning",
   "metadata": {},
   "source": [
    "N그램은 단어장 생성에 사용할 토큰의 크기를 결정.\n",
    "\n",
    "- 모노그램(monogram) : 토큰 하나만 단어로 사용\n",
    "- 바이그램(bigram) : 두 개의 연결된 토큰을 하나의 단어로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "unlikely-cornell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:41:32.807714Z",
     "start_time": "2021-08-30T03:41:32.798773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 12,\n",
       " 'is the': 2,\n",
       " 'the first': 7,\n",
       " 'first document': 1,\n",
       " 'the second': 9,\n",
       " 'second second': 6,\n",
       " 'second document': 5,\n",
       " 'and the': 0,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'is this': 3,\n",
       " 'this the': 13,\n",
       " 'the last': 8,\n",
       " 'last document': 4}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두개의 단어를 하나의 토큰으로 사용\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "referenced-opportunity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:43:11.556111Z",
     "start_time": "2021-08-30T03:43:11.534171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram_range(1, 2)는 모노그램, 바이그램 둘 다 사용하겠다.\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-hotel",
   "metadata": {},
   "source": [
    "# 빈도수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-captain",
   "metadata": {},
   "source": [
    "- `max_df` : 지정한 값보다 큰 빈도로 사용된 단어는 불용어처리\n",
    "- `min_df` : 지정한 값보다 작은 빈도로 사용된 단어는 불용어처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "enclosed-handling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:44:47.501166Z",
     "start_time": "2021-08-30T03:44:47.484213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'this': 3, 'is': 2, 'first': 1, 'document': 0},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "transsexual-arthur",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T03:44:53.144849Z",
     "start_time": "2021-08-30T03:44:53.126897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-newcastle",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-inspector",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency - Inverse Document) 인코딩 방법은 BOW 방법과는 다르게 모든 문서에 대해 공통적인 단어는 문서 구별 능력이 떨어진다고 보며 가중치를 축소하는 인코딩 방식이다.\n",
    "\n",
    "예를 들어 관사와 같이 a, an, the와 같은 단어는 모든 문서에서 공통적으로 발견되기 때문에 오히려 문서 구별 능력이 떨어진다고 보는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-dance",
   "metadata": {},
   "source": [
    "책마다 TF-IDF를 계산하는 방식은 약간씩 다르게 소개하나 sklearn에서는 다음의 방법으로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-average",
   "metadata": {},
   "source": [
    "$$\\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-romantic",
   "metadata": {},
   "source": [
    "idf는 df(document frequency)의 역수로 이에 log를 취해 스케일링한 값이 된다.\n",
    "\n",
    "즉, 문서 출현 빈도가 높을수록 idf값은 작아지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-teacher",
   "metadata": {},
   "source": [
    "$$\\text{idf}(d, t) = \\log \\dfrac{n}{1 + \\text{df}(t)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "familiar-windsor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T04:04:06.700742Z",
     "start_time": "2021-08-30T04:04:06.681735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
       "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
       "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
       "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
       "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-invitation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T04:04:06.747222Z",
     "start_time": "2021-08-30T04:04:06.733219Z"
    }
   },
   "source": [
    "# Hashing Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-senior",
   "metadata": {},
   "source": [
    "CountVectorizer의 경우 메모리상에서 작업을 수행하기에 처리할 문서의 크기가 커지면 컴퓨터 자원에 부담이 되기 마련이다.\n",
    "\n",
    "이 때 HashingVectorizer를 사용하면 해시함수를 통해 각 단어에 대한 index 번호를 생성해 메모리 및 실행시간이 단축되는 장점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "organizational-newspaper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T04:07:43.764329Z",
     "start_time": "2021-08-30T04:07:43.516589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "native-studio",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T04:07:52.432345Z",
     "start_time": "2021-08-30T04:07:47.429363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.99 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "single-quebec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T04:07:52.735043Z",
     "start_time": "2021-08-30T04:07:52.720081Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "buried-hello",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T04:08:00.963115Z",
     "start_time": "2021-08-30T04:07:58.927138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x300000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1786336 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time hv.transform(twenty.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
